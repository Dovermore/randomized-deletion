import time
from math import inf
from typing import Callable, Optional, List, Union

import torch
from scipy.special import gammaln
from torch.utils.data import Dataset
from math import ceil

from .perturbation import RandomPerturbation
from .certified_malconv import CertifiedMalConv

from ..models import Classifier
from ..utils import collate_pad
from ..types import IntBinarySample


def combln(n, k) -> float:
    return gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)


def brute_force_solve(f: Callable[[int], float], x_max: Optional[int] = None) -> int:
    """Find the largest non-negative integer argument of a decreasing function, such that its output remains positive

    Note:
    The solution is found by brute force: testing each value of the argument starting from 0.

    Args:
        f: A real-valued decreasing function, whose domain is the non-negative integers.
        x_max: Upper bound on the domain of `f`.

    Returns:
        The argument that satisfies the constraint. If the function is never positive, a value of -1 is returned.
    """
    x = -1
    while (x + 1 <= x_max if x_max else True) and f(x + 1) > 0:
        x += 1
    return x


def _exponential_bound(
    f: Callable[[int], float], base: Optional[int] = 4, x_max: Optional[int] = None
) -> int:
    # This assumes f(-1) > 0
    if base <= 1:
        raise ValueError("Base value have to be larger than 1")
    x = 1
    while (x_max is None or x <= x_max) and f(x) > 0:
        x *= base
    if x_max:
        x = min(x, x_max)
    return x


def binary_search_solve(f: Callable[[int], float], x_max: Optional[int] = None) -> int:
    """Find the largest non-negative integer argument of a decreasing function, such that its output remains positive
    using binary search

    Note:
    The solution is found by binary search: the upper value is specified by x_max.

    Args:
        f: A real-valued decreasing function, whose domain is the non-negative integers.
        x_max: Upper bound on the domain of `f`.

    Returns:
        The argument that satisfies the constraint. If the function is never positive, a value of -1 is returned.
    """
    x_left, f_left = -1, 1
    x_max_bound = _exponential_bound(f, base=4, x_max=x_max)
    x_max = min(x_max_bound, inf if x_max is None else x_max)
    x_right, f_right = x_max, f(x_max)

    # The maximum value is still negative, return the left x value
    if f(x_left + 1) < 0:
        return x_left
    # The minimum value is still positive, return the right x value
    elif f_right > 0:
        return x_right

    # Stop when left = right - 1, return left
    while x_left < x_right - 1:
        x_mid = (x_right + x_left) // 2
        f_mid = f(x_mid)
        if f_mid <= 0:
            x_right, f_right = x_mid, f_mid
        elif f_mid > 0:
            x_left, f_left = x_mid, f_mid
        else:
            raise ValueError("Nan detected when computing")
    assert f_left > 0 and f_right <= 0 and x_left == x_right - 1, "BS error"
    return x_left


def repeat_forward(
    sample: IntBinarySample,
    classifier: Classifier,
    perturbation: RandomPerturbation,
    num_samples: int,
    batch_size: int = 1,
    device: Union[torch.device, str, None] = None,
    verbose: int = 0,
) -> torch.Tensor:
    binary, metadata = sample

    classifier.to(device)
    classifier.eval()

    if verbose:
        print(f"Forward count: {num_samples}")

    if verbose:
        t0 = time.time()

    with torch.no_grad():
        # Pass through one sample to get the shape of the model's output
        binary = binary.unsqueeze(0).to(device)
        num_classes = classifier(binary).size(1)

        # Allocate tensor to count the predicted labels for each instance
        repeat_probs = torch.zeros(
            (num_samples, num_classes), dtype=torch.float, device=device
        )

        idx = 0
        num_samples_remain = num_samples
        for _ in range(ceil(num_samples_remain / batch_size)):
            if verbose:
                tl = time.time()

            this_batch_size = min(batch_size, num_samples_remain)
            num_samples_remain -= this_batch_size

            binary_rep = torch.repeat_interleave(binary, this_batch_size, dim=0)
            metadata_rep = collate_pad([metadata] * this_batch_size)
            binary_rep = perturbation((binary_rep, metadata_rep))

            probs = classifier.predict_proba(binary_rep)

            if verbose:
                tf = time.time()

            repeat_probs[idx : (idx + this_batch_size)] = probs
            idx += this_batch_size

            if verbose:
                tc = time.time()

                dt = time.time() - t0
                tot = (time.time() - t0) / idx * num_samples
                print(
                    f"\tcnt-{idx}-tot-{num_samples}-time-{dt:.4f}-tforward-{tf-tl:.4f}-tcount-{tc-tf:.4f}-tot-{tot:.4f}",
                    end="\r",
                    flush=True,
                )

    return repeat_probs


def repeat_forward_ds(
    dataset: Dataset,
    classifier: CertifiedMalConv,
    num_samples: int,
    batch_size: int = 1,
    device: Union[torch.device, str, None] = None,
    verbose: int = 0,
) -> torch.Tensor:
    """Repeat a dataset and compute the predicted probabilies in a 3d tensor

    Args:
        dataset: A dataset of samples to certify. Note that each sample should only consist of an input, i.e. no
            target, which can be passed directly to `perturbation`.
        classifier: Base classifier.
        perturbation: Random perturbation that is applied to raw inputs before being passed to the base classifier.
        num_samples: Number of Monte Carlo samples to use.

    Keyword args:
        batch_size: Number of samples to pass to the classifier in one call when computing expectations.
        device: Device used for the computation.
        verbose: Set logging level for this function (for debugging)

    Returns:
        Tensor with shape `(repeat, len(dataset), num_classes)`
    """
    classifier.to(device)
    classifier.eval()

    # Allocate tensor later once we know num_classes
    repeat_probs = None

    if verbose:
        print(f"Dataset size: {len(dataset)}")

    for i, sample in enumerate(dataset):
        classifier.reduce = "none"
        binaries, metadata = sample
        binaries = binaries.unsqueeze(0).to(device)
        metadata = collate_pad([metadata]).to(device)
        with torch.no_grad():
            repeat_probs_i = classifier.forward(
                binaries,
                num_samples=num_samples,
                return_logits=False,
                return_radii=False,
                batch_size=batch_size,
                forward_kwargs=dict(metadata=metadata),
            ).squeeze(1).cpu()

        if repeat_probs is None:
            num_classes = repeat_probs_i.size(1)
            repeat_probs = torch.empty(
                (num_samples, len(dataset), num_classes),
                dtype=torch.float,
            )

        repeat_probs[:, i, :] = repeat_probs_i

    return repeat_probs