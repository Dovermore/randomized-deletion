# Adapted from randomizedAblation/utils.py
from typing import Any, Optional, Tuple, Union
from math import ceil

import torch
from torch.utils.data import Dataset

from ..models import Classifier
from .perturbation import RandomPerturbation
from ..utils import collate_pad
from ..types import IntBinarySample


def avg_hard_forward(
    sample: IntBinarySample,
    classifier: Classifier,
    perturbation: RandomPerturbation,
    num_samples: int,
    batch_size: int = 1,
    device: Union[torch.device, str, None] = None,
    normalize: bool = False,
) -> torch.IntTensor:
    """Approximates the expected output of a smoothed classifier using Monte Carlo sampling

    Args:
        sample: Test sample to classify.
        classifier: Base classifier.
        perturbation: Random perturbation that is applied to raw inputs before being passed to the base classifier.
        num_samples: Number of Monte Carlo samples to use.

    Keyword args:
        batch_size: Number of samples to pass to the classifier in one call when the expected output.
        device: Device used for the computation.
        normalize: Returns class frequencies if False, otherwise returns probabilities.

    Returns:
        Tensor with shape (num_classes,)
    """
    classifier.to(device)
    classifier.eval()

    binary, metadata = sample

    with torch.no_grad():
        # Pass through one sample to get the shape of the classifier's output
        binary = binary.unsqueeze(0).to(device)
        num_classes = classifier(binary).size(1)

        # Allocate tensor to count the predicted labels
        label_counts = torch.zeros(num_classes, dtype=torch.int32, device=device)

        num_samples_remain = num_samples
        for _ in range(ceil(num_samples_remain / batch_size)):
            this_batch_size = min(batch_size, num_samples_remain)
            num_samples_remain -= this_batch_size

            binary_rep = torch.repeat_interleave(binary, this_batch_size, dim=0)
            metadata_rep = collate_pad([metadata] * this_batch_size)
            binary_rep = perturbation((binary_rep, metadata_rep))

            preds = classifier.predict(binary_rep)
            # Add to counts
            label_counts.index_put_(
                (preds,), torch.ones_like(preds, dtype=torch.int32), accumulate=True
            )

    # Divide by number of samples to get empirical expectation
    return label_counts / num_samples if normalize else label_counts


def certify(
    sample: IntBinarySample,
    classifier: Classifier,
    perturbation: RandomPerturbation,
    alpha: float,
    num_samples_pred: int,
    num_samples_bound: int,
    pred: Optional[int] = None,
    abstain: Optional[float] = None,
    batch_size: int = 1,
    return_counts: bool = False,
    device: Union[torch.device, str, None] = None,
    verbose: int = 0,
    **kwargs,
) -> Union[
    Tuple[int, float], Tuple[int, float, Tuple[torch.IntTensor, torch.IntTensor]]
]:
    """Certify a smoothed classifier

    Args:
        sample: Test sample to certify.
        classifier: Base classifier.
        perturbation: Random perturbation that is applied to raw inputs before being passed to the base classifier.
        alpha: Significance level.
        num_samples_pred: Number of samples used to estimate the prediction of the smoothed classifier.
        num_samples_bound: Number of samples used to estimate bound.

    Keyword args:
        pred: Known prediction of the smoothed classifier. If specified, the prediction will not be estimated again by
            Monte Carlo sampling.
        abstain: If specified, the smoothed classifier will abstain from making a prediction if the certified radius
            is less than or equal to this value.
        batch_size: Number of samples to pass to the classifier in one call when computing expectations.
        return_counts: If the count data should be returned. This option is added to help reuse some calculation.
        verbose: Set logging level for this function (for debugging)
        device: Device used for the computation.
        **kwargs: Keyword arguments passed to `perturbation.certified_radius` method

    Returns:
        A tuple containing the predicted class and the radius of certification. A predicted class of "-1" indicates
        an abstained prediction.
    """
    # Estimate class probabilities for the smoothed classifier using Monte Carlo samples. These samples are used
    # solely to estimate predictions.
    if pred is not None:
        label_counts_pred = avg_hard_forward(
            sample,
            classifier,
            perturbation,
            num_samples_pred,
            batch_size=batch_size,
            device=device,
            normalize=False,
        ).cpu()

        pred, pval = perturbation.predict(sample, label_counts_pred)

    # Estimate class probabilities for the smoothed classifier using more (independent) Monte Carlo samples. These
    # samples are used solely to estimate a lower bound on the probability of the predicted class (denoted
    # \underbar{p_A} in the paper).
    label_counts_bound = avg_hard_forward(
        sample,
        classifier,
        perturbation,
        num_samples_bound,
        batch_size=batch_size,
        device=device,
        normalize=False,
    ).cpu()

    radius = perturbation.certified_radius(
        sample, pred, label_counts_bound, alpha=alpha, **kwargs
    )

    if abstain is not None:
        # Abstain from making a prediction if the largest radius of certification is zero
        if radius <= abstain:
            pred = -1

    out = (pred, radius)
    if return_counts:
        out += ((label_counts_pred, label_counts_bound),)
    return out


def certify_ds(
    dataset: Dataset,
    classifier: Classifier,
    perturbation: RandomPerturbation,
    alpha: float,
    num_samples_pred: int,
    num_samples_bound: int,
    abstain: Optional[float] = None,
    batch_size: int = 1,
    return_counts: bool = False,
    device: Union[torch.device, str, None] = None,
    verbose: int = 0,
    **kwargs,
) -> Union[
    Tuple[torch.Tensor, torch.Tensor],
    Tuple[torch.Tensor, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
]:
    """Certify a smoothed classifier

    Args:
        dataset: A dataset of samples to certify. Note that each sample should only consist of an input, i.e. no
            target, which can be passed directly to `perturbation`.
        classifier: Base classifier.
        perturbation: Random perturbation that is applied to raw inputs before being passed to the base classifier.
        alpha: Significance level.
        num_samples_pred: Number of samples used to estimate the prediction of the smoothed classifier.
        num_samples_bound: Number of samples used to estimate bound.

    Keyword args:
        abstain: If specified, the smoothed classifier will abstain from making a prediction if the certified radius
            is less than or equal to this value.
        batch_size: Number of samples to pass to the classifier in one call when computing expectations.
        return_counts: If the count data should be returned. This option is added to help reuse some calculation.
        verbose: Set logging level for this function (for debugging).
        device: Device used for the computation.
        **kwargs: Keyword arguments passed to `perturbation.certified_radius` method

    Returns:
        A tuple of tensors where the first tensor contains the predicted classes and the second tensor contains the
        radii of certification. A predicted class of "-1" indicates an abstained prediction.
    """
    num_instances = len(dataset)

    preds = -torch.ones(num_instances, dtype=torch.int64, device=device)
    radii = torch.empty(num_instances, dtype=torch.float32, device=device)

    label_counts_pred = None
    label_counts_bound = None

    for i, sample in enumerate(dataset):
        # COMMENT: Metadata cannot be dropped here. Thus we should not apply transform to x (before this, x have to go through __call__ which ensures its BinarySample)
        result = certify(
            sample,
            classifier,
            perturbation,
            alpha,
            num_samples_pred,
            num_samples_bound,
            abstain=abstain,
            batch_size=batch_size,
            return_counts=return_counts,
            device=device,
            **kwargs,
        )

        if return_counts:
            preds[i], radii[i], label_counts_pred_i, label_counts_bound_i = result
            if label_counts_pred is None:
                num_classes = label_counts_pred_i.size(0)
                label_counts_pred = torch.empty(
                    (num_instances, num_classes), dtype=torch.int32, device=device
                )
                label_counts_bound = torch.empty(
                    (num_instances, num_classes), dtype=torch.int32, device=device
                )
            label_counts_pred[i] = label_counts_pred_i
            label_counts_bound[i] = label_counts_bound_i
        else:
            preds[i], radii[i] = result

    out = (preds, radii)
    if return_counts:
        out += ((label_counts_pred, label_counts_bound),)
    return out


def predict(
    sample: IntBinarySample,
    classifier: Classifier,
    perturbation: RandomPerturbation,
    alpha: float,
    num_samples: int,
    batch_size: int = 1,
    return_counts: bool = False,
    device: Union[torch.device, str, None] = None,
    verbose: int = 0,
) -> Union[int, Tuple[int, torch.IntTensor]]:
    """Predict

    Args:
        sample: Test sample.
        classifier: Base classifier.
        perturbation: Random perturbation that is applied to raw inputs before being passed to the base classifier.
        alpha: Significance level.
        num_samples: Number of Monte Carlo samples used to estimate predictions.

    Keyword args:
        batch_size: Number of samples to pass to the classifier in one call when computing expectations.
        return_counts: If the count data should be returned. This option is added to help reuse some calculation.
        device: Device used for the computation.
        verbose: Set logging level for this function (for debugging).

    Returns:

    """
    binary, _ = sample

    label_counts = avg_hard_forward(
        sample,
        classifier,
        perturbation,
        num_samples,
        batch_size=batch_size,
        device=device,
        normalize=False,
    ).cpu()

    pred, pvalue = perturbation.predict(binary, label_counts)
    if pvalue > alpha:
        pred = -1

    out = (pred,)
    if return_counts:
        out += (label_counts,)
    return out if len(out) > 1 else out[0]


def predict_ds(
    dataset: Dataset,
    classifier: Classifier,
    perturbation: RandomPerturbation,
    alpha: float,
    num_samples: int,
    batch_size: int = 1,
    return_counts: bool = False,
    device: Union[torch.device, str, None] = None,
    verbose: int = 0,
) -> Union[torch.LongTensor, Tuple[torch.LongTensor, torch.IntTensor]]:
    """Predict

    Args:
        dataset: A dataset of samples on which to make predictions. Note that each sample should only consist of an
            input, i.e. no target, which can be passed directly to `perturbation`.
        classifier: Base classifier.
        perturbation: Random perturbation that is applied to raw inputs before being passed to the base classifier.
        alpha: Significance level.
        num_samples: Number of Monte Carlo samples used to estimate predictions.

    Keyword args:
        abstain: If specified, the smoothed classifier will abstain from making a prediction if the certified radius
            is less than or equal to this value.
        batch_size: Number of samples to pass to the classifier in one call when computing expectations.
        return_counts: If the count data should be returned. This option is added to help reuse some calculation.
        verbose: Set logging level for this function (for debugging).
        device: Device used for the computation.
        **kwargs: Keyword arguments passed to `perturbation.certified_radius` method

    Returns:

    """
    num_instances = len(dataset)

    preds = -torch.ones(num_instances, dtype=torch.int64, device=device)
    label_counts = None

    for i, sample in enumerate(dataset):
        result = predict(
            sample,
            classifier,
            perturbation,
            alpha,
            num_samples,
            batch_size=batch_size,
            return_counts=return_counts,
            device=device,
            verbose=verbose,
        )

        if return_counts:
            preds[i], label_counts_i = result
            if label_counts is None:
                num_classes = label_counts_i.size(0)
                label_counts = torch.empty(
                    (num_instances, num_classes), dtype=torch.int32, device=device
                )
            label_counts[i] = label_counts_i
        else:
            preds[i] = result

    out = (preds,)
    if return_counts:
        out += (label_counts,)
    return out if len(out) > 1 else out[0]
